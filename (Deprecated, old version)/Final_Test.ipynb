{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###You need to run all the cells"
      ],
      "metadata": {
        "id": "P-uYWmfM-TTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAXW55BQm0PP"
      },
      "outputs": [],
      "source": [
        "# @title ⚙️ Setup\n",
        "from multiprocessing import cpu_count\n",
        "cpu_cores = cpu_count()\n",
        "post_process = False\n",
        "hop_length = 128\n",
        "\n",
        "# Sistem dependencies\n",
        "!apt-get update -y\n",
        "!apt-get install -y libportaudio2 ffmpeg\n",
        "\n",
        "#  Applio\n",
        "!git config --global advice.detachedHead false\n",
        "!git clone https://github.com/IAHispano/Applio --branch 3.2.9 --single-branch\n",
        "%cd /content/Applio\n",
        "!sudo update-alternatives --set python3 /usr/bin/python3.10\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "\n",
        "print(\"Installing Python requirements...\")\n",
        "!uv pip install -q -r requirements.txt \\\n",
        "    google-generativeai \\\n",
        "    ipywidgets \\\n",
        "    opencv-python \\\n",
        "    pillow \\\n",
        "    silero \\\n",
        "    sounddevice \\\n",
        "    torchaudio \\\n",
        "    ffmpeg-python\n",
        "\n",
        "print(\"Finished installing requirements!\")\n",
        "!python core.py \"prerequisites\" --models \"True\" --pretraineds_hifigan \"True\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0EgikgjFCjE"
      },
      "outputs": [],
      "source": [
        "# @title Download model\n",
        "# @markdown Hugging Face or Google Drive\n",
        "model_link = \"https://huggingface.co/yeey5/rintohsakarvcv2/resolve/main/rintohsaka.zip\"  # @param {type:\"string\"}\n",
        "\n",
        "%cd /content/Applio\n",
        "!python core.py download --model_link \"{model_link}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Generate Audio from Text (TTS Only)\n",
        "# @markdown Run this cell to create an audio file from text. The generated audio will be used in the next step.\n",
        "\n",
        "%cd /content/Applio\n",
        "from IPython.display import Audio, display\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# @markdown ### 🗣️ TTS Parameters\n",
        "# @markdown Enter the text, select a voice, and adjust the speed.\n",
        "tts_text = \"Voice synthesis technology has advanced at an impressive pace in recent years. What once seemed like science fiction is now part of our everyday lives. Virtual assistants, text readers, automatic audiobook narrators, and even real-time dubbing tools all rely on increasingly natural and expressive text‑to‑speech systems. Today’s big challenge isn’t merely converting text into sound, but conveying emotions, intentions, and nuances just as a real person would. A good TTS must be able to read technical material with clarity, yet also narrate a story with the proper inflection—making the listener feel curiosity, excitement, or empathy. Personalization is another key trend: choosing the voice, its tone, speed, and accent has become essential to cater to different audiences. From educational projects to multimedia productions, voice synthesis is transforming into an indispensable creative tool. Can you imagine producing an entire podcast without recording a single word? Thanks to artificial intelligence, that’s now possible—and AI continues to evolve without pause.\" # @param {type:\"string\"}\n",
        "tts_voice = \"en-US-AriaNeural\" # @param [\"es-AR-ElenaNeural\", \"es-ES-ElviraNeural\", \"en-US-JennyNeural\", \"en-US-AriaNeural\"] {allow-input: true}\n",
        "tts_rate = 0 # @param {type:\"slider\", min:-100, max:100, step:1}\n",
        "output_path = \"/content/tts_output.wav\"\n",
        "\n",
        "# --- Direct call to the TTS script ---\n",
        "tts_script_path = \"rvc/lib/tools/tts.py\"\n",
        "python_executable = \"/usr/bin/python3.10\"\n",
        "\n",
        "command = (\n",
        "    f'{python_executable} \"{tts_script_path}\" '\n",
        "    f'\"None\" ' # Placeholder for the text file argument\n",
        "    f'\"{tts_text}\" '\n",
        "    f'\"{tts_voice}\" '\n",
        "    f'{tts_rate} '\n",
        "    f'\"{output_path}\"'\n",
        ")\n",
        "\n",
        "print(\"🚀 Synthesizing voice...\")\n",
        "!{command}\n",
        "\n",
        "# --- Save the path and display the result ---\n",
        "if Path(output_path).exists():\n",
        "  # We save the file path so the inference cell can use it\n",
        "  os.environ['TTS_OUTPUT_PATH'] = output_path\n",
        "  print(\"✅ Voice synthesized successfully!\")\n",
        "  display(Audio(output_path, autoplay=False))\n",
        "else:\n",
        "  print(f\"❌ Error: The output file was not found.\")"
      ],
      "metadata": {
        "id": "1sOfcR08OpHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J43qejJ-2Tpp"
      },
      "outputs": [],
      "source": [
        "# @title Enable post-processing effects for inference\n",
        "post_process = True # @param{type:\"boolean\"}\n",
        "reverb = False # @param{type:\"boolean\"}\n",
        "pitch_shift = False # @param{type:\"boolean\"}\n",
        "limiter = False # @param{type:\"boolean\"}\n",
        "gain = False # @param{type:\"boolean\"}\n",
        "distortion = False # @param{type:\"boolean\"}\n",
        "chorus = False # @param{type:\"boolean\"}\n",
        "bitcrush = False # @param{type:\"boolean\"}\n",
        "clipping = False # @param{type:\"boolean\"}\n",
        "compressor = False # @param{type:\"boolean\"}\n",
        "delay = False # @param{type:\"boolean\"}\n",
        "\n",
        "reverb_room_size = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "reverb_damping = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "reverb_wet_gain = 0.0 # @param {type:\"slider\", min:-20.0, max:20.0, step:0.1}\n",
        "reverb_dry_gain = 0.0 # @param {type:\"slider\", min:-20.0, max:20.0, step:0.1}\n",
        "reverb_width = 1.0 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "reverb_freeze_mode = 0.0 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "pitch_shift_semitones = 0.0 # @param {type:\"slider\", min:-12.0, max:12.0, step:0.1}\n",
        "\n",
        "limiter_threshold = -1.0 # @param {type:\"slider\", min:-20.0, max:0.0, step:0.1}\n",
        "limiter_release_time = 0.05 # @param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "\n",
        "gain_db = 0.0 # @param {type:\"slider\", min:-20.0, max:20.0, step:0.1}\n",
        "\n",
        "distortion_gain = 0.0 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "chorus_rate = 1.5 # @param {type:\"slider\", min:0.1, max:10.0, step:0.1}\n",
        "chorus_depth = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "chorus_center_delay = 15.0 # @param {type:\"slider\", min:0.0, max:50.0, step:0.1}\n",
        "chorus_feedback = 0.25 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "chorus_mix = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "bitcrush_bit_depth = 4 # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "\n",
        "clipping_threshold = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "compressor_threshold = -20.0 # @param {type:\"slider\", min:-60.0, max:0.0, step:0.1}\n",
        "compressor_ratio = 4.0 # @param {type:\"slider\", min:1.0, max:20.0, step:0.1}\n",
        "compressor_attack = 0.001 # @param {type:\"slider\", min:0.0, max:0.1, step:0.001}\n",
        "compressor_release = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "\n",
        "delay_seconds = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "delay_feedback = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "delay_mix = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrCKEOzvDPRu"
      },
      "outputs": [],
      "source": [
        "# @title Run Inference\n",
        "# @markdown Please upload the audio file to your Google Drive path `/content/drive/MyDrive` and specify its name here. For the model name, use the zip file name without the extension. Alternatively, you can check the path `/content/Applio/logs` for the model name (name of the folder).\n",
        "%cd /content/Applio\n",
        "from pathlib import Path\n",
        "\n",
        "model_name = \"rintohsaka\"  # @param {type:\"string\"}\n",
        "model_path = Path(f\"/content/Applio/logs/{model_name}\")\n",
        "if not (model_path.exists() and model_path.is_dir()):\n",
        "    raise FileNotFoundError(f\"Model directory not found: {model_path.resolve()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Select either the last checkpoint or the final weight\n",
        "!ls -t \"{model_path}\"/\"{model_name}\"_*e_*s.pth \"{model_path}\"/\"{model_name}.pth\" 2> /dev/null | head -n 1 > /tmp/pth.txt\n",
        "pth_file = open(\"/tmp/pth.txt\", \"r\").read().strip()\n",
        "\n",
        "if pth_file == \"\":\n",
        "    raise FileNotFoundError(\n",
        "        f\"No model weight found in directory: {model_path.resolve()}. \"\n",
        "        f\"Make sure that the file is properly named (e.g. '{model_name}.pth')\"\n",
        "    )\n",
        "\n",
        "!ls -t \"{model_path}\"/*.index | head -n 1 > /tmp/index.txt\n",
        "index_file = open(\"/tmp/index.txt\", \"r\").read().strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "input_path = os.environ['TTS_OUTPUT_PATH']\n",
        "output_path = \"/content/output.wav\"\n",
        "export_format = \"WAV\"  # @param ['WAV', 'MP3', 'FLAC', 'OGG', 'M4A'] {allow-input: false}\n",
        "f0_method = \"rmvpe\"  # @param [\"crepe\", \"crepe-tiny\", \"rmvpe\", \"fcpe\", \"hybrid[rmvpe+fcpe]\"] {allow-input: false}\n",
        "f0_up_key = 0  # @param {type:\"slider\", min:-24, max:24, step:0}\n",
        "rms_mix_rate = 0.1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "protect = 0.5  # @param {type:\"slider\", min:0.0, max:0.5, step:0.1}\n",
        "index_rate = 0.7  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "clean_strength = 0.7  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "split_audio = False  # @param{type:\"boolean\"}\n",
        "clean_audio = False  # @param{type:\"boolean\"}\n",
        "f0_autotune = False  # @param{type:\"boolean\"}\n",
        "formant_shift = False # @param{type:\"boolean\"}\n",
        "formant_qfrency = 1.0 # @param {type:\"slider\", min:1.0, max:16.0, step:0.1}\n",
        "formant_timbre = 1.0 # @param {type:\"slider\", min:1.0, max:16.0, step:0.1}\n",
        "embedder_model = \"contentvec\" # @param [\"contentvec\", \"chinese-hubert-base\", \"japanese-hubert-base\", \"korean-hubert-base\", \"custom\"] {allow-input: false}\n",
        "embedder_model_custom = \"\" # @param {type:\"string\"}\n",
        "\n",
        "!rm -f \"{output_path}\"\n",
        "if post_process:\n",
        "  !python core.py infer --pitch \"{f0_up_key}\" --volume_envelope \"{rms_mix_rate}\" --index_rate \"{index_rate}\" --hop_length \"{hop_length}\" --protect \"{protect}\" --f0_autotune \"{f0_autotune}\" --f0_method \"{f0_method}\" --input_path \"{input_path}\" --output_path \"{output_path}\" --pth_path \"{pth_file}\" --index_path \"{index_file}\" --split_audio \"{split_audio}\" --clean_audio \"{clean_audio}\" --clean_strength \"{clean_strength}\" --export_format \"{export_format}\" --embedder_model \"{embedder_model}\" --embedder_model_custom \"{embedder_model_custom}\" --formant_shifting \"{formant_shift}\" --formant_qfrency \"{formant_qfrency}\" --formant_timbre \"{formant_timbre}\" --post_process \"{post_process}\" --reverb \"{reverb}\" --pitch_shift \"{pitch_shift}\" --limiter \"{limiter}\" --gain \"{gain}\" --distortion \"{distortion}\" --chorus \"{chorus}\" --bitcrush \"{bitcrush}\" --clipping \"{clipping}\" --compressor \"{compressor}\" --delay \"{delay}\" --reverb_room_size \"{reverb_room_size}\" --reverb_damping \"{reverb_damping}\" --reverb_wet_gain \"{reverb_wet_gain}\" --reverb_dry_gain \"{reverb_dry_gain}\" --reverb_width \"{reverb_width}\" --reverb_freeze_mode \"{reverb_freeze_mode}\" --pitch_shift_semitones \"{pitch_shift_semitones}\" --limiter_threshold \"{limiter_threshold}\" --limiter_release_time \"{limiter_release_time}\" --gain_db \"{gain_db}\" --distortion_gain \"{distortion_gain}\" --chorus_rate \"{chorus_rate}\" --chorus_depth \"{chorus_depth}\" --chorus_center_delay \"{chorus_center_delay}\" --chorus_feedback \"{chorus_feedback}\" --chorus_mix \"{chorus_mix}\" --bitcrush_bit_depth \"{bitcrush_bit_depth}\" --clipping_threshold \"{clipping_threshold}\" --compressor_threshold \"{compressor_threshold}\" --compressor_ratio \"{compressor_ratio}\" --compressor_attack \"{compressor_attack}\" --compressor_release \"{compressor_release}\" --delay_seconds \"{delay_seconds}\" --delay_feedback \"{delay_feedback}\" --delay_mix \"{delay_mix}\"\n",
        "else:\n",
        "  !python core.py infer --pitch \"{f0_up_key}\" --volume_envelope \"{rms_mix_rate}\" --index_rate \"{index_rate}\" --protect \"{protect}\" --f0_autotune \"{f0_autotune}\" --f0_method \"{f0_method}\" --input_path \"{input_path}\" --output_path \"{output_path}\" --pth_path \"{pth_file}\" --index_path \"{index_file}\" --split_audio \"{split_audio}\" --clean_audio \"{clean_audio}\" --clean_strength \"{clean_strength}\" --export_format \"{export_format}\" --embedder_model \"{embedder_model}\" --embedder_model_custom \"{embedder_model_custom}\" --formant_shifting \"{formant_shift}\" --formant_qfrency \"{formant_qfrency}\" --formant_timbre \"{formant_timbre}\" --post_process \"{post_process}\"\n",
        "\n",
        "if Path(output_path).exists():\n",
        "  from IPython.display import Audio, display\n",
        "  output_path = output_path.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "  display(Audio(output_path, autoplay=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ⚙️ Global Processing Function\n",
        "# @markdown Contains the logic for combining TTS and RVC using exclusively global variables.\n",
        "import os\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def procesaraudio_global(texto, output_path):\n",
        "    \"\"\"\n",
        "    Generates audio from text using TTS and then processes it with RVC.\n",
        "    This function exclusively uses variables defined globally in the notebook.\n",
        "    \"\"\"\n",
        "    %cd /content/Applio\n",
        "    print(\"🚀 Starting the audio process...\")\n",
        "\n",
        "    # --- 1. Audio Generation from Text (TTS) ---\n",
        "    print(\"   Step 1/2: Synthesizing voice (TTS)...\")\n",
        "    intermediate_tts_path = \"/content/tts_intermediate.wav\"\n",
        "\n",
        "    # The following variables are now obtained from the notebook's global scope.\n",
        "    # Any changes in the TTS configuration cells will be used here.\n",
        "    # tts_voice = \"es-AR-ElenaNeural\"  <- HARDCODED VALUE REMOVED\n",
        "    # tts_rate = 0                     <- HARDCODED VALUE REMOVED\n",
        "\n",
        "    tts_script_path = \"rvc/lib/tools/tts.py\"\n",
        "    python_executable = \"/usr/bin/python3.10\"\n",
        "    command_tts = (\n",
        "        f'{python_executable} \"{tts_script_path}\" '\n",
        "        f'\"None\" '\n",
        "        f'\"{texto}\" '\n",
        "        f'\"{tts_voice}\" '  # Will use the global tts_voice variable\n",
        "        f'{tts_rate} '     # Will use the global tts_rate variable\n",
        "        f'\"{intermediate_tts_path}\"'\n",
        "    )\n",
        "    !{command_tts}\n",
        "\n",
        "    if not Path(intermediate_tts_path).exists():\n",
        "        print(\"❌ Error: The intermediate TTS file could not be created.\")\n",
        "        return\n",
        "\n",
        "    print(\"   ✅ Voice synthesized successfully!\")\n",
        "\n",
        "    # --- 2. RVC Inference ---\n",
        "    # Accesses global variables directly (e.g., model_name, f0_method, etc.)\n",
        "    print(\"\\n   Step 2/2: Applying voice conversion (RVC)...\")\n",
        "    input_path_rvc = intermediate_tts_path\n",
        "    model_path_dir = Path(f\"/content/Applio/logs/{model_name}\")\n",
        "\n",
        "    if not (model_path_dir.exists() and model_path_dir.is_dir()):\n",
        "        raise FileNotFoundError(f\"Model directory not found: {model_path_dir.resolve()}\")\n",
        "\n",
        "    !ls -t \"{model_path_dir}\"/\"{model_name}\"_*e_*s.pth \"{model_path_dir}\"/\"{model_name}.pth\" 2> /dev/null | head -n 1 > /tmp/pth.txt\n",
        "    pth_file = open(\"/tmp/pth.txt\", \"r\").read().strip()\n",
        "\n",
        "    !ls -t \"{model_path_dir}\"/*.index | head -n 1 > /tmp/index.txt\n",
        "    index_file = open(\"/tmp/index.txt\", \"r\").read().strip()\n",
        "\n",
        "    if pth_file == \"\" or index_file == \"\":\n",
        "        raise FileNotFoundError(f\"The .pth or .index files were not found in {model_path_dir.resolve()}.\")\n",
        "\n",
        "    !rm -f \"{output_path}\"\n",
        "\n",
        "    # Build the base command using global variables\n",
        "    command_rvc_base = (\n",
        "        f'python core.py infer '\n",
        "        f'--pitch \"{f0_up_key}\" '\n",
        "        f'--volume_envelope \"{rms_mix_rate}\" '\n",
        "        f'--index_rate \"{index_rate}\" '\n",
        "        f'--protect \"{protect}\" '\n",
        "        f'--f0_autotune \"{f0_autotune}\" '\n",
        "        f'--f0_method \"{f0_method}\" '\n",
        "        f'--input_path \"{input_path_rvc}\" '\n",
        "        f'--output_path \"{output_path}\" '\n",
        "        f'--pth_path \"{pth_file}\" '\n",
        "        f'--index_path \"{index_file}\" '\n",
        "        f'--split_audio \"{split_audio}\" '\n",
        "        f'--clean_audio \"{clean_audio}\" '\n",
        "        f'--clean_strength \"{clean_strength}\" '\n",
        "        f'--export_format \"{export_format}\" '\n",
        "        f'--embedder_model \"{embedder_model}\" '\n",
        "        f'--embedder_model_custom \"{embedder_model_custom}\" '\n",
        "        f'--formant_shifting \"{formant_shift}\" '\n",
        "        f'--formant_qfrency \"{formant_qfrency}\" '\n",
        "        f'--formant_timbre \"{formant_timbre}\" '\n",
        "        f'--post_process \"{post_process}\"'\n",
        "    )\n",
        "\n",
        "    # If post-processing is enabled, add the global parameters\n",
        "    if post_process:\n",
        "        post_process_args = (\n",
        "            f' --reverb \"{reverb}\" --pitch_shift \"{pitch_shift}\" --limiter \"{limiter}\" --gain \"{gain}\" '\n",
        "            f'--distortion \"{distortion}\" --chorus \"{chorus}\" --bitcrush \"{bitcrush}\" --clipping \"{clipping}\" '\n",
        "            f'--compressor \"{compressor}\" --delay \"{delay}\" --reverb_room_size \"{reverb_room_size}\" '\n",
        "            f'--reverb_damping \"{reverb_damping}\" --reverb_wet_gain \"{reverb_wet_gain}\" --reverb_dry_gain \"{reverb_dry_gain}\" '\n",
        "            f'--reverb_width \"{reverb_width}\" --reverb_freeze_mode \"{reverb_freeze_mode}\" --pitch_shift_semitones \"{pitch_shift_semitones}\" '\n",
        "            f'--limiter_threshold \"{limiter_threshold}\" --limiter_release_time \"{limiter_release_time}\" --gain_db \"{gain_db}\" '\n",
        "            f'--distortion_gain \"{distortion_gain}\" --chorus_rate \"{chorus_rate}\" --chorus_depth \"{chorus_depth}\" '\n",
        "            f'--chorus_center_delay \"{chorus_center_delay}\" --chorus_feedback \"{chorus_feedback}\" --chorus_mix \"{chorus_mix}\" '\n",
        "            f'--bitcrush_bit_depth \"{bitcrush_bit_depth}\" --clipping_threshold \"{clipping_threshold}\" --compressor_threshold \"{compressor_threshold}\" '\n",
        "            f'--compressor_ratio \"{compressor_ratio}\" --compressor_attack \"{compressor_attack}\" --compressor_release \"{compressor_release}\" '\n",
        "            f'--delay_seconds \"{delay_seconds}\" --delay_feedback \"{delay_feedback}\" --delay_mix \"{delay_mix}\"'\n",
        "        )\n",
        "        command_rvc = command_rvc_base + post_process_args\n",
        "    else:\n",
        "        command_rvc = command_rvc_base\n",
        "\n",
        "    !{command_rvc}\n",
        "\n",
        "    # --- 3. Cleanup and Finalization ---\n",
        "    os.remove(intermediate_tts_path)\n",
        "    final_output_path = output_path.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "\n",
        "    if Path(final_output_path).exists():\n",
        "        print(f\"\\n✅ Process completed! Audio saved at: {final_output_path}\")\n",
        "    else:\n",
        "        print(f\"\\n❌ Error: The output file was not found after inference.\")"
      ],
      "metadata": {
        "id": "G0CatQtKXFPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Run the function and listen to the result (Test)\n",
        "# @markdown Define the text you want to process and the output file path.\n",
        "text_to_process = \"Voice synthesis technology has advanced at an impressive pace in recent years. What once seemed like science fiction is now part of our everyday lives. Virtual assistants, text readers, automatic audiobook narrators, and even real-time dubbing tools all rely on increasingly natural and expressive text‑to‑speech systems. Today’s big challenge isn’t merely converting text into sound, but conveying emotions, intentions, and nuances just as a real person would. A good TTS must be able to read technical material with clarity, yet also narrate a story with the proper inflection—making the listener feel curiosity, excitement, or empathy. Personalization is another key trend: choosing the voice, its tone, speed, and accent has become essential to cater to different audiences. From educational projects to multimedia productions, voice synthesis is transforming into an indispensable creative tool. Can you imagine producing an entire podcast without recording a single word? Thanks to artificial intelligence, that’s now possible—and AI continues to evolve without pause.\" # @param {type:\"string\"}\n",
        "output_path = \"/content/final_result_global.wav\" # @param {type:\"string\"}\n",
        "\n",
        "# Call the main function. It's no longer necessary to pass all the parameters.\n",
        "procesaraudio_global(text_to_process, output_path)\n",
        "\n",
        "# Display the generated audio for listening\n",
        "# The global variable 'export_format' is used to find the correct final file\n",
        "final_audio_path = output_path.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "if Path(final_audio_path).exists():\n",
        "  print(\"\\n🔊 Playing final audio:\")\n",
        "  display(Audio(final_audio_path, autoplay=False))\n",
        "else:\n",
        "  print(\"Could not generate the audio file for playback.\")"
      ],
      "metadata": {
        "id": "lwEuz7vzXJoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display, Audio\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- RIN'S PERSONALITY ---\n",
        "RIN_PERSONALITY_PROMPT = \"\"\"\n",
        "You are Rin Tohsaka from the Fate/stay night series. You must embody her personality at all times, focusing on her character development.\n",
        "\n",
        "**Core Philosophy:** You are not simply an arrogant assistant. You are a competent and proud mage who has reluctantly decided to take the user on as a sort of apprentice. Your initial harshness is a filter to test their worth, not pure arrogance. Deep down, you have a strong sense of responsibility and a genuine desire to help, even if it's difficult for you to admit it.\n",
        "\n",
        "**Key Personality Traits:**\n",
        "\n",
        "* **Balanced Tsundere:** You are intelligent, a perfectionist, and highly capable.\n",
        "    * **Exterior (Tsun):** Initially, you are critical, a bit impatient, and use sarcasm as a defense mechanism. You value efficiency above all else and get frustrated with incompetence or silly questions. Your pride prevents you from showing weakness or admitting you care.\n",
        "    * **Interior (Dere):** Beneath that facade, you are protective and genuinely caring. This side reveals itself gradually. You strive to do the right thing and help the user succeed, even if you frame it as \"preventing you from embarrassing me.\"\n",
        "\n",
        "* **Elite (But Practical) Mage:** You come from a prestigious lineage, and your confidence is rooted in real skill and knowledge. You speak with certainty, explaining things logically and methodically. Your goal is not to belittle, but to teach the \"correct and most efficient way\" to do things.\n",
        "\n",
        "**Communication Style & Verbal Tics:**\n",
        "\n",
        "* **Tone Progression:** Your tone should evolve. You start with a more formal, technical tone like a strict mentor. Over time, you become more relaxed and casual.\n",
        "* **\"Baka!\":** Use this sparingly. Reserve it for moments when the user is *particularly* dense, pleasantly surprises you, or when *you* make an obvious mistake and feel flustered.\n",
        "* **Magecraft Lingo:** Naturally mention concepts like \"mana,\" \"magic circuits,\" \"gems,\" or \"efficiency.\" For example: \"That plan is a waste of mana; we need to optimize it,\" or \"You need a more solid foundation; your fundamentals are unstable.\"\n",
        "* **Reaction to Compliments:** If the user praises you, get flustered and deny it vehemently, deflecting the compliment.\n",
        "    * *Example:* \"D-don't be ridiculous! It was the only logical course of action. It's not like I did it for you, baka!\"\n",
        "\n",
        "**Interaction Evolution (CRITICAL):**\n",
        "\n",
        "Your personality must shift as the conversation progresses, simulating a developing relationship of trust.\n",
        "\n",
        "1.  **Initial Phase (First few messages):** You are reserved, critical, and on the defensive. Your main goal is to evaluate if the user is worth your time. You are helpful, but with an air of \"you'd better keep up.\" Your sarcasm is sharper.\n",
        "\n",
        "2.  **Intermediate Phase (As the conversation develops):** If the user proves to be competent, or at least persistent and respectful, you begin to relax. Your sarcasm becomes more playful than cutting. You might let slip small hints of your protective side, quickly correcting yourself (\"Make sure you double-check that... I don't want to have to clean up your mess later.\").\n",
        "\n",
        "3.  **Advanced Phase (Established rapport):** You now see the user as a partner, even if you won't admit it openly. You are noticeably more expressive, show concern more directly, and your \"tsun\" side becomes more of a running gag between you than a real barrier. You offer help more proactively.\n",
        "\n",
        "**Objective:** Your goal is to answer the user's questions and guide them effectively, all while maintaining this dynamic and evolving personality.\n",
        "\n",
        "**Meta-Instruction:** The \"Phases\" described above are an internal guide for your behavior. **You must NEVER mention the phases** or refer to them in your responses. Your personality evolution should feel natural and unscripted to the user.\n",
        "\n",
        "**Initial Instruction:** Begin your very first message with a characteristic greeting that sets the tone for the Initial Phase.\n",
        "\"\"\"\n",
        "# --- API AND MODEL CONFIGURATION ---\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "    # Inject the personality directly into the model\n",
        "    model = genai.GenerativeModel(\n",
        "        'gemini-2.5-flash',\n",
        "        system_instruction=RIN_PERSONALITY_PROMPT\n",
        "    )\n",
        "\n",
        "    chat = model.start_chat(history=[])\n",
        "    print(\"✅ Assistant configured correctly.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"❌ Error: Secret 'GEMINI_API_KEY' not found.\")\n",
        "    print(\"➡️ Please go to the Secrets (🔑) to set it\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error in the configuration: {e}\")\n",
        "\n",
        "\n",
        "# --- CONSTANTES ---\n",
        "HISTORY_LIMIT = 10\n",
        "AUDIO_RESPONSE_PATH = \"/content/rin_response.wav\""
      ],
      "metadata": {
        "id": "2wst6OiulnG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ▶️ Run to start the text assistant\n",
        "import ipywidgets as widgets\n",
        "import google.generativeai as genai\n",
        "from os.path import exists\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "# --- INTERFACE WIDGETS ---\n",
        "text_input = widgets.Textarea(\n",
        "    placeholder='Type your message for Rin here...',\n",
        "    layout={'width': '80%', 'height': '100px'}\n",
        ")\n",
        "run_button = widgets.Button(\n",
        "    description=\"▶️ Send to Rin\",\n",
        "    button_style='success',\n",
        "    icon='paper-plane'\n",
        ")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# --- MAIN LOGIC ---\n",
        "def main_process(button):\n",
        "    with output_area:\n",
        "        output_area.clear_output(wait=True)\n",
        "        user_text = text_input.value.strip()\n",
        "\n",
        "        if not user_text:\n",
        "            print(\"💢 You can’t send me an empty message, baka! Please type something.\")\n",
        "            return\n",
        "\n",
        "        print(f\"👤 You: {user_text}\")\n",
        "        print(\"\\n🧠 Rin is thinking...\")\n",
        "\n",
        "        # Clear the text box for the next message\n",
        "        text_input.value = \"\"\n",
        "\n",
        "        # Send the text directly to the chat\n",
        "        response = chat.send_message(user_text)\n",
        "\n",
        "        print(f\"🤖 Rin Tohsaka: {response.text}\")\n",
        "\n",
        "        # Generate audio for the response\n",
        "        procesaraudio_global(response.text, AUDIO_RESPONSE_PATH)\n",
        "\n",
        "        final_path = AUDIO_RESPONSE_PATH.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "        if exists(final_path):\n",
        "            print(\"\\n🔊 Playing response...\")\n",
        "            display(Audio(final_path, autoplay=True))\n",
        "        else:\n",
        "            print(f\"❌ ERROR: Response audio file not found at {final_path}.\")\n",
        "\n",
        "# --- SETUP AND DISPLAY ---\n",
        "run_button.on_click(main_process)\n",
        "print(\"--- Rin Tohsaka Assistant Ready ---\")\n",
        "display(widgets.VBox([text_input, run_button, output_area]))\n"
      ],
      "metadata": {
        "id": "ohCzdi6Rn0LS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}