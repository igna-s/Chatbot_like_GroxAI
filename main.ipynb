{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5cjKcMA3w5e"
      },
      "source": [
        "# üöÄ General Environment Setup\n",
        "This section handles the complete one-time setup. It installs all required dependencies, system libraries, and downloads the necessary models to run the project.\n",
        "If you run this locally, you will skip this part next time (Use Venv if possible)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7csj8pBn08Cg",
        "outputId": "b0089520-96cf-42b5-a82f-2ea4258522fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Done\n"
          ]
        }
      ],
      "source": [
        "#@title Setup Environment\n",
        "import os\n",
        "\n",
        "# Clone OpenVoice repository\n",
        "!git clone -q https://github.com/myshell-ai/OpenVoice.git > /dev/null 2>&1\n",
        "os.chdir('OpenVoice')\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q nltk flask pyngrok flask-cors waitress faster-whisper pydub torch torchvision torchaudio \"google-generativeai\" -e . \"git+https://github.com/myshell-ai/MeloTTS.git\" --extra-index-url https://download.pytorch.org/whl/cu118 > /dev/null 2>&1\n",
        "\n",
        "# Download NLTK data\n",
        "!python -m nltk.downloader -q punkt averaged_perceptron_tagger > /dev/null 2>&1\n",
        "!python -m nltk.downloader -q averaged_perceptron_tagger_eng > /dev/null 2>&1\n",
        "\n",
        "\n",
        "# Download and set up pre-trained models\n",
        "!wget -q -O checkpoints_v2.zip https://myshell-public-repo-host.s3.amazonaws.com/openvoice/checkpoints_v2_0417.zip > /dev/null 2>&1\n",
        "!unzip -q -o checkpoints_v2.zip -d checkpoints_v2 > /dev/null 2>&1\n",
        "!mv /content/OpenVoice/checkpoints_v2/checkpoints_v2/* /content/OpenVoice/checkpoints_v2/ > /dev/null 2>&1\n",
        "!rmdir /content/OpenVoice/checkpoints_v2/checkpoints_v2 > /dev/null 2>&1\n",
        "!rm checkpoints_v2.zip > /dev/null 2>&1\n",
        "\n",
        "# Download unidic dictionary for MeloTTS\n",
        "!python -m unidic download > /dev/null 2>&1\n",
        "\n",
        "print(\"‚úÖ Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q_s2lmr2k-X",
        "outputId": "0c5a70f7-08d3-4bab-fca9-db90d19805cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "‚úÖ System libraries OK\n"
          ]
        }
      ],
      "source": [
        "#@title Setup System Libraries (GPU bug correction)\n",
        "import os\n",
        "\n",
        "package_name = \"libcudnn8\"\n",
        "required_version = \"8.9.2.26-1+cuda11.8\"\n",
        "\n",
        "check_command = f\"dpkg-query -W -f='{{Status}} {{Version}}' {package_name} 2>/dev/null | grep -q 'install ok installed {required_version}'\"\n",
        "\n",
        "if os.system(check_command) != 0:\n",
        "    !apt-get update -qq > /dev/null\n",
        "    !apt-get install -y -qq libcusparse-dev-11-8 libcublas-dev-11-8 libcudnn8={required_version} > /dev/null\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] = os.environ.get('LD_LIBRARY_PATH', '') + ':/usr/lib/x86_64-linux-gnu'\n",
        "\n",
        "print(\"‚úÖ System libraries OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZY6BPy62Ka7"
      },
      "outputs": [],
      "source": [
        "#@title Verify NumPy Version (Bug correction, Colab will restart)\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "required_version = \"1.26.4\"\n",
        "\n",
        "if np.__version__ != required_version:\n",
        "    !pip install numpy=={required_version} --quiet > /dev/null 2>&1\n",
        "    os.kill(os.getpid(), 9)\n",
        "else:\n",
        "    print(\"‚úÖ NumPy OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dHLudKq4NDy"
      },
      "source": [
        "# ‚ñ∂Ô∏è Run the Application\n",
        "This is the final step. Execute the following cells to launch the main program. A public URL will be generated in the output below, allowing you to access the interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BYKQtSY4TG9",
        "outputId": "29bbda6c-776b-4705-ed57-451c762206e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Assets already exist in /content/. Skipping ZIP upload.\n",
            "‚úÖ Active AI Personality Language: English\n",
            "\n",
            "‚úÖ Google API Key configured.\n",
            "\n",
            "üß† Loading all AI models (this may take a moment)...\n",
            "‚úÖ Selected device: cuda:0\n",
            "üß† Configuring Gemini...\n",
            "‚úÖ Gemini model ready.\n",
            "üß† Loading OpenVoice...\n",
            "/content/OpenVoice\n",
            "Loaded checkpoint '/content/OpenVoice/checkpoints_v2/converter/checkpoint.pth'\n",
            "missing/unexpected keys: [] []\n",
            "‚úÖ OpenVoice model loaded.\n",
            "üß† Loading Faster Whisper...\n",
            "‚úÖ Faster Whisper model loaded.\n",
            "üß† Loading MeloTTS in 'English'...\n",
            "‚úÖ MeloTTS model loaded.\n",
            "\n",
            "üîä Processing fixed voice sample from 'reference.mp3'...\n",
            "OpenVoice version: v2\n",
            "[(1.582, 3.026), (10.03, 16.082), (17.006, 22.29)]\n",
            "after vad: dur = 12.78\n",
            "‚úÖ Voice timbre successfully extracted. System is ready to run!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import gc\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import google.generativeai as genai\n",
        "from openvoice import se_extractor\n",
        "from openvoice.api import ToneColorConverter\n",
        "from melo.api import TTS\n",
        "from faster_whisper import WhisperModel\n",
        "import nltk\n",
        "\n",
        "\n",
        "# --- 1. UPLOAD & EXTRACT ASSETS ZIP ---\n",
        "# This block runs only if the asset files don't already exist to prevent re-uploads.\n",
        "if not os.path.exists('/content/cliente_final.html'):\n",
        "    print(\"üëá Please upload your 'assets.zip' file.\")\n",
        "    print(\"   This ZIP should contain: cliente_final.html, reference.mp3, and all .vrm, .fbx, .hdr files.\")\n",
        "    uploaded_zip = files.upload()\n",
        "\n",
        "    if not uploaded_zip:\n",
        "        raise Exception(\"‚ùå No ZIP file was uploaded. Please run the cell again.\")\n",
        "    else:\n",
        "        zip_name = list(uploaded_zip.keys())[0]\n",
        "        print(f\"\\n‚úÖ File '{zip_name}' uploaded. Extracting to /content/...\")\n",
        "        with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/')\n",
        "        os.remove(zip_name) # Clean up the zip file after extraction\n",
        "        print(\"‚úÖ Assets successfully extracted.\")\n",
        "else:\n",
        "    print(\"‚úÖ Assets already exist in /content/. Skipping ZIP upload.\")\n",
        "\n",
        "\n",
        "# @title üîë API Key, AI Personality & Language Configuration\n",
        "# @markdown Paste your API keys here. They are required for Google AI and Ngrok services.\n",
        "GOOGLE_API_KEY = \"\" #@param {type:\"string\"}\n",
        "NGROK_AUTHTOKEN = \"\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#  AI Personality & Language Configuration\n",
        "# @markdown Define the AI's personality for each language and set the active audio language.\n",
        "SYSTEM_PROMPT_ES = \"Eres un asistente de IA conversacional. Tu personalidad es la de una chica de anime amigable y servicial. Responde siempre de forma breve y directa en espa\\u00F1ol. Tus respuestas no deben exceder las dos frases. S\\u00E9 muy concisa.\" #@param {type:\"string\"}\n",
        "SYSTEM_PROMPT_EN = \"You are a conversational AI assistant. Your personality is that of a friendly and helpful anime girl. Always respond briefly and directly in English. Your answers should not exceed two sentences. Be very concise.\" #@param {type:\"string\"}\n",
        "TTS_LANGUAGE = \"English\" #@param [\"Spanish\", \"English\"]\n",
        "\n",
        "\n",
        "# --- Select the active system prompt based on the chosen language ---\n",
        "ACTIVE_SYSTEM_PROMPT = SYSTEM_PROMPT_EN if TTS_LANGUAGE == \"English\" else SYSTEM_PROMPT_ES\n",
        "print(f\"‚úÖ Active AI Personality Language: {TTS_LANGUAGE}\")\n",
        "\n",
        "# --- API Validation & Setup ---\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "print(\"\\n‚úÖ Google API Key configured.\")\n",
        "\n",
        "# --- 4. LOAD AI MODELS ---\n",
        "print(\"\\nüß† Loading all AI models (this may take a moment)...\")\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"‚úÖ Selected device: {device}\")\n",
        "\n",
        "# Gemini Model (now using the DYNAMIC system prompt)\n",
        "print(\"üß† Configuring Gemini...\")\n",
        "gemini_model = genai.GenerativeModel('gemini-2.5-flash', system_instruction=ACTIVE_SYSTEM_PROMPT)\n",
        "chat = gemini_model.start_chat(history=[])\n",
        "print(\"‚úÖ Gemini model ready.\")\n",
        "\n",
        "# OpenVoice Model\n",
        "print(\"üß† Loading OpenVoice...\")\n",
        "# Change directory to ensure relative paths for models are correct\n",
        "%cd /content/OpenVoice\n",
        "ckpt_converter = '/content/OpenVoice/checkpoints_v2/converter'\n",
        "tone_color_converter = ToneColorConverter(f'{ckpt_converter}/config.json', device=device)\n",
        "tone_color_converter.load_ckpt(f'{ckpt_converter}/checkpoint.pth')\n",
        "print(\"‚úÖ OpenVoice model loaded.\")\n",
        "\n",
        "# Faster Whisper Model\n",
        "print(\"üß† Loading Faster Whisper...\")\n",
        "compute_type = \"float16\" if \"cuda\" in device else \"int8\"\n",
        "whisper_model = WhisperModel(\"medium\", device=device.split(':')[0], compute_type=compute_type)\n",
        "print(\"‚úÖ Faster Whisper model loaded.\")\n",
        "\n",
        "# MeloTTS Model (using the configured language)\n",
        "print(f\"üß† Loading MeloTTS in '{TTS_LANGUAGE}'...\")\n",
        "LANGUAGE_CODE_MAP = {\n",
        "    \"Spanish\": \"ES\",\n",
        "    \"English\": \"EN\"\n",
        "}\n",
        "selected_language = LANGUAGE_CODE_MAP[TTS_LANGUAGE]\n",
        "\n",
        "melo_model = TTS(language=selected_language, device=device)\n",
        "speaker_ids = melo_model.hps.data.spk2id\n",
        "print(\"‚úÖ MeloTTS model loaded.\")\n",
        "\n",
        "# --- 5. EXTRACT VOICE TIMBRE FROM REFERENCE FILE ---\n",
        "print(\"\\nüîä Processing fixed voice sample from 'reference.mp3'...\")\n",
        "reference_speaker_path = \"/content/reference.mp3\"\n",
        "\n",
        "if not os.path.exists(reference_speaker_path):\n",
        "    raise Exception(f\"‚ùå File not found: '{reference_speaker_path}'. Ensure it was included in your ZIP file.\")\n",
        "else:\n",
        "    target_se, _ = se_extractor.get_se(\n",
        "        reference_speaker_path,\n",
        "        tone_color_converter,\n",
        "        target_dir='/content/OpenVoice/processed',\n",
        "        vad=True\n",
        "    )\n",
        "    print(\"‚úÖ Voice timbre successfully extracted. System is ready to run!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "168039f5d9d842f5af9dfc3a4bfc51dc",
            "8b840d7200f4461393d9461aea481607",
            "524e66737b144fe897799c88ff52ac3e",
            "b012d1633e3645c9a5ea55eb2a3dce92",
            "bb2ce96b458c4fb8b1ae9f0e76d608e8",
            "5ce116bb3d9b405a8362b01f0c65104e",
            "d450fcd787ce479488d213b511de2d37",
            "0361330ce58649fd934aeab860545203",
            "f3ac2b4801504898b0ef546fc03c0d03",
            "63ebea26c45648bdaa23246cd0f3eb43",
            "6b25963d43134e1aa8a813678f40d2e1"
          ]
        },
        "id": "10jZ0ZSp5IB7",
        "outputId": "56725dfe-f2ac-4a22-bed9-47f0fa98a40b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Configuring Flask server...\n",
            "‚úÖ Ngrok region set to: South America (Sao Paulo) (sa)\n",
            "‚úÖ Flask server initialized with CORS.\n",
            "\n",
            "==================================================\n",
            "üî• Performing a warm-up call to pre-compile AI models...\n",
            "   - Using warm-up text: 'Initializing systems.'\n",
            "   - Using voice: EN-US\n",
            " > Text split to sentences.\n",
            "Initializing systems.\n",
            " > ===========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "168039f5d9d842f5af9dfc3a4bfc51dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:12<00:00, 12.99s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîä Audio generated and saved to: /content/warmup_response.wav\n",
            "   - Warm-up file cleaned up.\n",
            "‚úÖ Models are now warmed up and ready for real-time requests.\n",
            "\n",
            "==================================================\n",
            "üöá Starting ngrok tunnel...\n",
            "‚úÖ SERVER IS READY AND RUNNING\n",
            "üîó Public URL: NgrokTunnel: \"https://1c23886c7fe7.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "   Open this URL in your browser!\n",
            "==================================================\n",
            "\n",
            "‚öôÔ∏è  Starting production server with Waitress...\n",
            "\n",
            "üé§ Request d2feda9c-f19c-435c-981c-004136966ee9 received...\n",
            "   - Step 1: Audio received in memory (66815 bytes).\n",
            "   - Step 2: Converted to WAV format in memory.\n",
            "   - Step 3: Temp WAV file saved for processing: /content/d2feda9c-f19c-435c-981c-004136966ee9_input.wav\n",
            "   - Whisper language set to: 'en'\n",
            "   - Step 4: Transcribed text: 'Good morning, Vietnam!'\n",
            "üß† Sending to Gemini: 'Good morning, Vietnam!'\n",
            "   - Using voice: EN-US\n",
            " > Text split to sentences.\n",
            "Good morning to you too! Have a nice day!\n",
            " > ===========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîä Audio generated and saved to: /content/d2feda9c-f19c-435c-981c-004136966ee9_response.wav\n",
            "‚úÖ Sending response: {'audio_file': 'd2feda9c-f19c-435c-981c-004136966ee9_response.wav', 'animation_file': 'anim_3.fbx'}\n",
            "üßπ Clearing memory for request d2feda9c-f19c-435c-981c-004136966ee9\n",
            "\n",
            "üé§ Request 54c9c4ca-5e8f-43de-baf1-677e11bf5a59 received...\n",
            "   - Step 1: Audio received in memory (89033 bytes).\n",
            "   - Step 2: Converted to WAV format in memory.\n",
            "   - Step 3: Temp WAV file saved for processing: /content/54c9c4ca-5e8f-43de-baf1-677e11bf5a59_input.wav\n",
            "   - Whisper language set to: 'en'\n",
            "   - Step 4: Transcribed text: 'Well, well seems like this works'\n",
            "üß† Sending to Gemini: 'Well, well seems like this works'\n",
            "   - Using voice: EN-US\n",
            " > Text split to sentences.\n",
            "Great! Is there anything I can help you with today?\n",
            " > ===========================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîä Audio generated and saved to: /content/54c9c4ca-5e8f-43de-baf1-677e11bf5a59_response.wav\n",
            "‚úÖ Sending response: {'audio_file': '54c9c4ca-5e8f-43de-baf1-677e11bf5a59_response.wav', 'animation_file': 'anim_3.fbx'}\n",
            "üßπ Clearing memory for request 54c9c4ca-5e8f-43de-baf1-677e11bf5a59\n"
          ]
        }
      ],
      "source": [
        "#@title ‚ñ∂Ô∏è Run Web Server & Application\n",
        "\n",
        "\n",
        "#@markdown ### ‚öôÔ∏è Server & Ngrok Configuration\n",
        "#@markdown Choose the server region closest to you for the best performance.\n",
        "SERVER_REGION = \"South America (Sao Paulo)\" #@param [\"United States (Ohio)\", \"Europe (Frankfurt)\", \"Asia/Pacific (Singapore)\", \"Australia (Sydney)\", \"South America (Sao Paulo)\", \"India (Mumbai)\", \"Japan (Tokyo)\"]\n",
        "\n",
        "import os\n",
        "import uuid\n",
        "import traceback\n",
        "import io\n",
        "import gc\n",
        "import random\n",
        "from flask import Flask, request, jsonify, send_from_directory\n",
        "from pyngrok import ngrok, conf\n",
        "from flask_cors import CORS\n",
        "from waitress import serve\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# --- 1. SERVER CONFIGURATION ---\n",
        "print(\"\\nüöÄ Configuring Flask server...\")\n",
        "\n",
        "# --- Map user-friendly region names to official ngrok codes ---\n",
        "REGION_MAP = {\n",
        "    \"United States (Ohio)\": \"us\",\n",
        "    \"Europe (Frankfurt)\": \"eu\",\n",
        "    \"Asia/Pacific (Singapore)\": \"ap\",\n",
        "    \"Australia (Sydney)\": \"au\",\n",
        "    \"South America (Sao Paulo)\": \"sa\",\n",
        "    \"India (Mumbai)\": \"in\",\n",
        "    \"Japan (Tokyo)\": \"jp\"\n",
        "}\n",
        "selected_region_code = REGION_MAP[SERVER_REGION]\n",
        "\n",
        "# --- Set ngrok configuration from user parameters ---\n",
        "conf.get_default().auth_token = NGROK_AUTHTOKEN\n",
        "conf.get_default().region = selected_region_code\n",
        "print(f\"‚úÖ Ngrok region set to: {SERVER_REGION} ({selected_region_code})\")\n",
        "\n",
        "ROOT_DIR = '/content/'\n",
        "app = Flask(__name__)\n",
        "# Allow all origins for Cross-Origin Resource Sharing (CORS)\n",
        "CORS(app, resources={r\"/*\": {\"origins\": \"*\"}})\n",
        "# Pre-defined list of animation files for random selection\n",
        "ANIMATION_FILES = [f'anim_{i}.fbx' for i in range(1, 4)]\n",
        "print(\"‚úÖ Flask server initialized with CORS.\")\n",
        "\n",
        "# --- 2. AI PROCESSING FUNCTIONS ---\n",
        "def reason_with_gemini(user_text):\n",
        "    \"\"\"Sends user text to the Gemini model and returns its response.\"\"\"\n",
        "    print(f\"üß† Sending to Gemini: '{user_text}'\")\n",
        "    try:\n",
        "        response = chat.send_message(user_text)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"üö® Gemini API Error: {e}\")\n",
        "        return \"Sorry, I encountered a problem while generating my response.\"\n",
        "\n",
        "def generate_cloned_audio(text, output_filename, request_id):\n",
        "    \"\"\"Generates cloned voice audio from text using MeloTTS and OpenVoice.\"\"\"\n",
        "    # Logic to select the correct voice model based on the chosen language\n",
        "    if selected_language == 'ES':\n",
        "        speaker_id_key = 'ES'\n",
        "        embedding_file = 'es'\n",
        "    else: # English\n",
        "        speaker_id_key = 'EN-US'\n",
        "        embedding_file = 'en-us'\n",
        "\n",
        "    print(f\"   - Using voice: {speaker_id_key}\")\n",
        "\n",
        "    src_path = os.path.join(ROOT_DIR, f\"{request_id}_tmp_melo.wav\")\n",
        "    save_path = os.path.join(ROOT_DIR, output_filename)\n",
        "\n",
        "    try:\n",
        "        # Load the base speaker embedding for the source voice\n",
        "        source_se = torch.load(f'/content/OpenVoice/checkpoints_v2/base_speakers/ses/{embedding_file}.pth', map_location=device)\n",
        "        # Generate the initial audio with MeloTTS\n",
        "        melo_model.tts_to_file(text, speaker_ids[speaker_id_key], src_path, speed=1.0)\n",
        "        # Convert the tone color to the target voice using OpenVoice\n",
        "        tone_color_converter.convert(\n",
        "            audio_src_path=src_path,\n",
        "            src_se=source_se,\n",
        "            tgt_se=target_se,\n",
        "            output_path=save_path,\n",
        "            message=\"@MyShell\"\n",
        "        )\n",
        "        print(f\"üîä Audio generated and saved to: {save_path}\")\n",
        "        return output_filename\n",
        "    finally:\n",
        "        # Ensure the temporary source file is always removed\n",
        "        if os.path.exists(src_path):\n",
        "            os.remove(src_path)\n",
        "\n",
        "# --- 3. API ROUTES (ENDPOINTS) ---\n",
        "@app.route('/')\n",
        "def index():\n",
        "    \"\"\"Serves the main HTML client file.\"\"\"\n",
        "    return send_from_directory(ROOT_DIR, 'cliente_final.html')\n",
        "\n",
        "@app.route('/assets/<path:filename>')\n",
        "def serve_asset(filename):\n",
        "    \"\"\"Serves static assets like audio files from the root directory.\"\"\"\n",
        "    return send_from_directory(ROOT_DIR, filename,\n",
        "                               mimetype='audio/wav' if filename.endswith('.wav') else None)\n",
        "\n",
        "@app.route('/process_audio', methods=['POST'])\n",
        "def process_audio_endpoint():\n",
        "    \"\"\"Main endpoint to process user audio, get a response, and return generated audio.\"\"\"\n",
        "    request_id = str(uuid.uuid4())\n",
        "    print(f\"\\nüé§ Request {request_id} received...\")\n",
        "\n",
        "    try:\n",
        "        if 'audio' not in request.files:\n",
        "            return jsonify({\"error\": \"No audio file found in the request\"}), 400\n",
        "\n",
        "        # --- IN-MEMORY PROCESSING ---\n",
        "        audio_data = request.files['audio'].read()\n",
        "        print(f\"   - Step 1: Audio received in memory ({len(audio_data)} bytes).\")\n",
        "\n",
        "        sound = AudioSegment.from_file(io.BytesIO(audio_data))\n",
        "        wav_buffer = io.BytesIO()\n",
        "        sound.export(wav_buffer, format=\"wav\")\n",
        "        wav_buffer.seek(0)\n",
        "        print(\"   - Step 2: Converted to WAV format in memory.\")\n",
        "\n",
        "        # --- TEMPORARY FILE FOR WHISPER ---\n",
        "        input_path = os.path.join(ROOT_DIR, f\"{request_id}_input.wav\")\n",
        "        with open(input_path, \"wb\") as f:\n",
        "            f.write(wav_buffer.getvalue())\n",
        "        print(f\"   - Step 3: Temp WAV file saved for processing: {input_path}\")\n",
        "\n",
        "        # --- DYNAMIC LANGUAGE CONFIGURATION FOR WHISPER ---\n",
        "        whisper_language_code = \"en\" if TTS_LANGUAGE == \"English\" else \"es\"\n",
        "        print(f\"   - Whisper language set to: '{whisper_language_code}'\")\n",
        "\n",
        "        # --- TRANSCRIPTION WITH FASTER-WHISPER  ---\n",
        "        segments, info = whisper_model.transcribe(\n",
        "            input_path,                   # The audio file to process.\n",
        "            beam_size=5,                  # Improves transcription accuracy.\n",
        "            language=whisper_language_code, # Sets the language ('en', 'es', etc.).\n",
        "            vad_filter=True               # Removes periods of silence/noise.\n",
        "        )\n",
        "\n",
        "        transcribed_text = \"\".join(seg.text for seg in segments).strip()\n",
        "        print(f\"   - Step 4: Transcribed text: '{transcribed_text}'\")\n",
        "\n",
        "        # --- CLEANUP INPUT FILE ---\n",
        "        os.remove(input_path)\n",
        "\n",
        "        if not transcribed_text:\n",
        "            return jsonify({\"error\": \"Could not detect any text in the audio.\"}), 400\n",
        "\n",
        "        # --- GENERATE RESPONSE ---\n",
        "        response_text = reason_with_gemini(transcribed_text)\n",
        "        output_filename = f\"{request_id}_response.wav\"\n",
        "        generated_file = generate_cloned_audio(\n",
        "            response_text, output_filename, request_id\n",
        "        )\n",
        "        if generated_file is None:\n",
        "            raise Exception(\"Failed to generate the final audio file.\")\n",
        "\n",
        "        response_data = {\n",
        "            \"audio_file\": generated_file,\n",
        "            \"animation_file\": random.choice(ANIMATION_FILES)\n",
        "        }\n",
        "        print(f\"‚úÖ Sending response: {response_data}\")\n",
        "        return jsonify(response_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üî• Endpoint Error: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return jsonify({\"error\": \"An internal server error occurred\"}), 500\n",
        "\n",
        "    finally:\n",
        "        # --- MEMORY RELEASE ---\n",
        "        print(f\"üßπ Clearing memory for request {request_id}\")\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# --- 4. WARM-UP CALL (TO PRE-COMPILE MODELS) ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üî• Performing a warm-up call to pre-compile AI models...\")\n",
        "# Select warm-up text based on the configured language\n",
        "warmup_text = \"Initializing systems.\" if TTS_LANGUAGE == \"English\" else \"Inicializando sistemas.\"\n",
        "print(f\"   - Using warm-up text: '{warmup_text}'\")\n",
        "try:\n",
        "    # Define dummy filenames for the warm-up call\n",
        "    warmup_output_file = \"warmup_response.wav\"\n",
        "    warmup_request_id = \"warmup_request\"\n",
        "\n",
        "    # Execute the main audio generation function\n",
        "    generated_file = generate_cloned_audio(\n",
        "        warmup_text,\n",
        "        warmup_output_file,\n",
        "        warmup_request_id\n",
        "    )\n",
        "    # Clean up the generated warm-up file\n",
        "    if generated_file and os.path.exists(os.path.join(ROOT_DIR, generated_file)):\n",
        "        os.remove(os.path.join(ROOT_DIR, generated_file))\n",
        "        print(\"   - Warm-up file cleaned up.\")\n",
        "    print(\"‚úÖ Models are now warmed up and ready for real-time requests.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warning: Warm-up call failed, the first user request might be slow. Error: {e}\")\n",
        "\n",
        "# --- 5. START SERVER ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üöá Starting ngrok tunnel...\")\n",
        "# Kill any existing ngrok tunnels before starting a new one\n",
        "ngrok.kill()\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"‚úÖ SERVER IS READY AND RUNNING\")\n",
        "print(f\"üîó Public URL: {public_url}\")\n",
        "print(\"   Open this URL in your browser!\")\n",
        "print(\"=\"*50)\n",
        "print(\"\\n‚öôÔ∏è  Starting production server with Waitress...\")\n",
        "serve(app, host='0.0.0.0', port=5000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0361330ce58649fd934aeab860545203": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "168039f5d9d842f5af9dfc3a4bfc51dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b840d7200f4461393d9461aea481607",
              "IPY_MODEL_524e66737b144fe897799c88ff52ac3e",
              "IPY_MODEL_b012d1633e3645c9a5ea55eb2a3dce92"
            ],
            "layout": "IPY_MODEL_bb2ce96b458c4fb8b1ae9f0e76d608e8"
          }
        },
        "524e66737b144fe897799c88ff52ac3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0361330ce58649fd934aeab860545203",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3ac2b4801504898b0ef546fc03c0d03",
            "value": 440449768
          }
        },
        "5ce116bb3d9b405a8362b01f0c65104e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63ebea26c45648bdaa23246cd0f3eb43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b25963d43134e1aa8a813678f40d2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b840d7200f4461393d9461aea481607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ce116bb3d9b405a8362b01f0c65104e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d450fcd787ce479488d213b511de2d37",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "b012d1633e3645c9a5ea55eb2a3dce92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63ebea26c45648bdaa23246cd0f3eb43",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6b25963d43134e1aa8a813678f40d2e1",
            "value": "‚Äá440M/440M‚Äá[00:06&lt;00:00,‚Äá55.4MB/s]"
          }
        },
        "bb2ce96b458c4fb8b1ae9f0e76d608e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d450fcd787ce479488d213b511de2d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3ac2b4801504898b0ef546fc03c0d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
