{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###You need to run all the cells"
      ],
      "metadata": {
        "id": "P-uYWmfM-TTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAXW55BQm0PP"
      },
      "outputs": [],
      "source": [
        "# @title ‚öôÔ∏è Setup\n",
        "from multiprocessing import cpu_count\n",
        "cpu_cores = cpu_count()\n",
        "post_process = False\n",
        "hop_length = 128\n",
        "\n",
        "# Sistem dependencies\n",
        "!apt-get update -y\n",
        "!apt-get install -y libportaudio2 ffmpeg\n",
        "\n",
        "#  Applio\n",
        "!git config --global advice.detachedHead false\n",
        "!git clone https://github.com/IAHispano/Applio --branch 3.2.9 --single-branch\n",
        "%cd /content/Applio\n",
        "!sudo update-alternatives --set python3 /usr/bin/python3.10\n",
        "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
        "\n",
        "\n",
        "print(\"Installing Python requirements...\")\n",
        "!uv pip install -q -r requirements.txt \\\n",
        "    google-generativeai \\\n",
        "    ipywidgets \\\n",
        "    opencv-python \\\n",
        "    pillow \\\n",
        "    silero \\\n",
        "    sounddevice \\\n",
        "    torchaudio \\\n",
        "    ffmpeg-python\n",
        "\n",
        "print(\"Finished installing requirements!\")\n",
        "!python core.py \"prerequisites\" --models \"True\" --pretraineds_hifigan \"True\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0EgikgjFCjE"
      },
      "outputs": [],
      "source": [
        "# @title Download model\n",
        "# @markdown Hugging Face or Google Drive\n",
        "model_link = \"https://huggingface.co/yeey5/rintohsakarvcv2/resolve/main/rintohsaka.zip\"  # @param {type:\"string\"}\n",
        "\n",
        "%cd /content/Applio\n",
        "!python core.py download --model_link \"{model_link}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Generate Audio from Text (TTS Only)\n",
        "# @markdown Run this cell to create an audio file from text. The generated audio will be used in the next step.\n",
        "\n",
        "%cd /content/Applio\n",
        "from IPython.display import Audio, display\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# @markdown ### üó£Ô∏è TTS Parameters\n",
        "# @markdown Enter the text, select a voice, and adjust the speed.\n",
        "tts_text = \"Voice synthesis technology has advanced at an impressive pace in recent years. What once seemed like science fiction is now part of our everyday lives. Virtual assistants, text readers, automatic audiobook narrators, and even real-time dubbing tools all rely on increasingly natural and expressive text‚Äëto‚Äëspeech systems. Today‚Äôs big challenge isn‚Äôt merely converting text into sound, but conveying emotions, intentions, and nuances just as a real person would. A good TTS must be able to read technical material with clarity, yet also narrate a story with the proper inflection‚Äîmaking the listener feel curiosity, excitement, or empathy. Personalization is another key trend: choosing the voice, its tone, speed, and accent has become essential to cater to different audiences. From educational projects to multimedia productions, voice synthesis is transforming into an indispensable creative tool. Can you imagine producing an entire podcast without recording a single word? Thanks to artificial intelligence, that‚Äôs now possible‚Äîand AI continues to evolve without pause.\" # @param {type:\"string\"}\n",
        "tts_voice = \"en-US-AriaNeural\" # @param [\"es-AR-ElenaNeural\", \"es-ES-ElviraNeural\", \"en-US-JennyNeural\", \"en-US-AriaNeural\"] {allow-input: true}\n",
        "tts_rate = 0 # @param {type:\"slider\", min:-100, max:100, step:1}\n",
        "output_path = \"/content/tts_output.wav\"\n",
        "\n",
        "# --- Direct call to the TTS script ---\n",
        "tts_script_path = \"rvc/lib/tools/tts.py\"\n",
        "python_executable = \"/usr/bin/python3.10\"\n",
        "\n",
        "command = (\n",
        "    f'{python_executable} \"{tts_script_path}\" '\n",
        "    f'\"None\" ' # Placeholder for the text file argument\n",
        "    f'\"{tts_text}\" '\n",
        "    f'\"{tts_voice}\" '\n",
        "    f'{tts_rate} '\n",
        "    f'\"{output_path}\"'\n",
        ")\n",
        "\n",
        "print(\"üöÄ Synthesizing voice...\")\n",
        "!{command}\n",
        "\n",
        "# --- Save the path and display the result ---\n",
        "if Path(output_path).exists():\n",
        "  # We save the file path so the inference cell can use it\n",
        "  os.environ['TTS_OUTPUT_PATH'] = output_path\n",
        "  print(\"‚úÖ Voice synthesized successfully!\")\n",
        "  display(Audio(output_path, autoplay=False))\n",
        "else:\n",
        "  print(f\"‚ùå Error: The output file was not found.\")"
      ],
      "metadata": {
        "id": "1sOfcR08OpHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J43qejJ-2Tpp"
      },
      "outputs": [],
      "source": [
        "# @title Enable post-processing effects for inference\n",
        "post_process = True # @param{type:\"boolean\"}\n",
        "reverb = False # @param{type:\"boolean\"}\n",
        "pitch_shift = False # @param{type:\"boolean\"}\n",
        "limiter = False # @param{type:\"boolean\"}\n",
        "gain = False # @param{type:\"boolean\"}\n",
        "distortion = False # @param{type:\"boolean\"}\n",
        "chorus = False # @param{type:\"boolean\"}\n",
        "bitcrush = False # @param{type:\"boolean\"}\n",
        "clipping = False # @param{type:\"boolean\"}\n",
        "compressor = False # @param{type:\"boolean\"}\n",
        "delay = False # @param{type:\"boolean\"}\n",
        "\n",
        "reverb_room_size = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "reverb_damping = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "reverb_wet_gain = 0.0 # @param {type:\"slider\", min:-20.0, max:20.0, step:0.1}\n",
        "reverb_dry_gain = 0.0 # @param {type:\"slider\", min:-20.0, max:20.0, step:0.1}\n",
        "reverb_width = 1.0 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "reverb_freeze_mode = 0.0 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "pitch_shift_semitones = 0.0 # @param {type:\"slider\", min:-12.0, max:12.0, step:0.1}\n",
        "\n",
        "limiter_threshold = -1.0 # @param {type:\"slider\", min:-20.0, max:0.0, step:0.1}\n",
        "limiter_release_time = 0.05 # @param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "\n",
        "gain_db = 0.0 # @param {type:\"slider\", min:-20.0, max:20.0, step:0.1}\n",
        "\n",
        "distortion_gain = 0.0 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "chorus_rate = 1.5 # @param {type:\"slider\", min:0.1, max:10.0, step:0.1}\n",
        "chorus_depth = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "chorus_center_delay = 15.0 # @param {type:\"slider\", min:0.0, max:50.0, step:0.1}\n",
        "chorus_feedback = 0.25 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "chorus_mix = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "bitcrush_bit_depth = 4 # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "\n",
        "clipping_threshold = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "compressor_threshold = -20.0 # @param {type:\"slider\", min:-60.0, max:0.0, step:0.1}\n",
        "compressor_ratio = 4.0 # @param {type:\"slider\", min:1.0, max:20.0, step:0.1}\n",
        "compressor_attack = 0.001 # @param {type:\"slider\", min:0.0, max:0.1, step:0.001}\n",
        "compressor_release = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "\n",
        "delay_seconds = 0.1 # @param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "delay_feedback = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "delay_mix = 0.5 # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrCKEOzvDPRu"
      },
      "outputs": [],
      "source": [
        "# @title Run Inference\n",
        "# @markdown Please upload the audio file to your Google Drive path `/content/drive/MyDrive` and specify its name here. For the model name, use the zip file name without the extension. Alternatively, you can check the path `/content/Applio/logs` for the model name (name of the folder).\n",
        "%cd /content/Applio\n",
        "from pathlib import Path\n",
        "\n",
        "model_name = \"rintohsaka\"  # @param {type:\"string\"}\n",
        "model_path = Path(f\"/content/Applio/logs/{model_name}\")\n",
        "if not (model_path.exists() and model_path.is_dir()):\n",
        "    raise FileNotFoundError(f\"Model directory not found: {model_path.resolve()}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Select either the last checkpoint or the final weight\n",
        "!ls -t \"{model_path}\"/\"{model_name}\"_*e_*s.pth \"{model_path}\"/\"{model_name}.pth\" 2> /dev/null | head -n 1 > /tmp/pth.txt\n",
        "pth_file = open(\"/tmp/pth.txt\", \"r\").read().strip()\n",
        "\n",
        "if pth_file == \"\":\n",
        "    raise FileNotFoundError(\n",
        "        f\"No model weight found in directory: {model_path.resolve()}. \"\n",
        "        f\"Make sure that the file is properly named (e.g. '{model_name}.pth')\"\n",
        "    )\n",
        "\n",
        "!ls -t \"{model_path}\"/*.index | head -n 1 > /tmp/index.txt\n",
        "index_file = open(\"/tmp/index.txt\", \"r\").read().strip()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "input_path = os.environ['TTS_OUTPUT_PATH']\n",
        "output_path = \"/content/output.wav\"\n",
        "export_format = \"WAV\"  # @param ['WAV', 'MP3', 'FLAC', 'OGG', 'M4A'] {allow-input: false}\n",
        "f0_method = \"rmvpe\"  # @param [\"crepe\", \"crepe-tiny\", \"rmvpe\", \"fcpe\", \"hybrid[rmvpe+fcpe]\"] {allow-input: false}\n",
        "f0_up_key = 0  # @param {type:\"slider\", min:-24, max:24, step:0}\n",
        "rms_mix_rate = 0.1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "protect = 0.5  # @param {type:\"slider\", min:0.0, max:0.5, step:0.1}\n",
        "index_rate = 0.7  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "clean_strength = 0.7  # @param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "split_audio = False  # @param{type:\"boolean\"}\n",
        "clean_audio = False  # @param{type:\"boolean\"}\n",
        "f0_autotune = False  # @param{type:\"boolean\"}\n",
        "formant_shift = False # @param{type:\"boolean\"}\n",
        "formant_qfrency = 1.0 # @param {type:\"slider\", min:1.0, max:16.0, step:0.1}\n",
        "formant_timbre = 1.0 # @param {type:\"slider\", min:1.0, max:16.0, step:0.1}\n",
        "embedder_model = \"contentvec\" # @param [\"contentvec\", \"chinese-hubert-base\", \"japanese-hubert-base\", \"korean-hubert-base\", \"custom\"] {allow-input: false}\n",
        "embedder_model_custom = \"\" # @param {type:\"string\"}\n",
        "\n",
        "!rm -f \"{output_path}\"\n",
        "if post_process:\n",
        "  !python core.py infer --pitch \"{f0_up_key}\" --volume_envelope \"{rms_mix_rate}\" --index_rate \"{index_rate}\" --hop_length \"{hop_length}\" --protect \"{protect}\" --f0_autotune \"{f0_autotune}\" --f0_method \"{f0_method}\" --input_path \"{input_path}\" --output_path \"{output_path}\" --pth_path \"{pth_file}\" --index_path \"{index_file}\" --split_audio \"{split_audio}\" --clean_audio \"{clean_audio}\" --clean_strength \"{clean_strength}\" --export_format \"{export_format}\" --embedder_model \"{embedder_model}\" --embedder_model_custom \"{embedder_model_custom}\" --formant_shifting \"{formant_shift}\" --formant_qfrency \"{formant_qfrency}\" --formant_timbre \"{formant_timbre}\" --post_process \"{post_process}\" --reverb \"{reverb}\" --pitch_shift \"{pitch_shift}\" --limiter \"{limiter}\" --gain \"{gain}\" --distortion \"{distortion}\" --chorus \"{chorus}\" --bitcrush \"{bitcrush}\" --clipping \"{clipping}\" --compressor \"{compressor}\" --delay \"{delay}\" --reverb_room_size \"{reverb_room_size}\" --reverb_damping \"{reverb_damping}\" --reverb_wet_gain \"{reverb_wet_gain}\" --reverb_dry_gain \"{reverb_dry_gain}\" --reverb_width \"{reverb_width}\" --reverb_freeze_mode \"{reverb_freeze_mode}\" --pitch_shift_semitones \"{pitch_shift_semitones}\" --limiter_threshold \"{limiter_threshold}\" --limiter_release_time \"{limiter_release_time}\" --gain_db \"{gain_db}\" --distortion_gain \"{distortion_gain}\" --chorus_rate \"{chorus_rate}\" --chorus_depth \"{chorus_depth}\" --chorus_center_delay \"{chorus_center_delay}\" --chorus_feedback \"{chorus_feedback}\" --chorus_mix \"{chorus_mix}\" --bitcrush_bit_depth \"{bitcrush_bit_depth}\" --clipping_threshold \"{clipping_threshold}\" --compressor_threshold \"{compressor_threshold}\" --compressor_ratio \"{compressor_ratio}\" --compressor_attack \"{compressor_attack}\" --compressor_release \"{compressor_release}\" --delay_seconds \"{delay_seconds}\" --delay_feedback \"{delay_feedback}\" --delay_mix \"{delay_mix}\"\n",
        "else:\n",
        "  !python core.py infer --pitch \"{f0_up_key}\" --volume_envelope \"{rms_mix_rate}\" --index_rate \"{index_rate}\" --protect \"{protect}\" --f0_autotune \"{f0_autotune}\" --f0_method \"{f0_method}\" --input_path \"{input_path}\" --output_path \"{output_path}\" --pth_path \"{pth_file}\" --index_path \"{index_file}\" --split_audio \"{split_audio}\" --clean_audio \"{clean_audio}\" --clean_strength \"{clean_strength}\" --export_format \"{export_format}\" --embedder_model \"{embedder_model}\" --embedder_model_custom \"{embedder_model_custom}\" --formant_shifting \"{formant_shift}\" --formant_qfrency \"{formant_qfrency}\" --formant_timbre \"{formant_timbre}\" --post_process \"{post_process}\"\n",
        "\n",
        "if Path(output_path).exists():\n",
        "  from IPython.display import Audio, display\n",
        "  output_path = output_path.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "  display(Audio(output_path, autoplay=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚öôÔ∏è Global Processing Function\n",
        "# @markdown Contains the logic for combining TTS and RVC using exclusively global variables.\n",
        "import os\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def procesaraudio_global(texto, output_path):\n",
        "    \"\"\"\n",
        "    Generates audio from text using TTS and then processes it with RVC.\n",
        "    This function exclusively uses variables defined globally in the notebook.\n",
        "    \"\"\"\n",
        "    %cd /content/Applio\n",
        "    print(\"üöÄ Starting the audio process...\")\n",
        "\n",
        "    # --- 1. Audio Generation from Text (TTS) ---\n",
        "    print(\"   Step 1/2: Synthesizing voice (TTS)...\")\n",
        "    intermediate_tts_path = \"/content/tts_intermediate.wav\"\n",
        "\n",
        "    # The following variables are now obtained from the notebook's global scope.\n",
        "    # Any changes in the TTS configuration cells will be used here.\n",
        "    # tts_voice = \"es-AR-ElenaNeural\"  <- HARDCODED VALUE REMOVED\n",
        "    # tts_rate = 0                     <- HARDCODED VALUE REMOVED\n",
        "\n",
        "    tts_script_path = \"rvc/lib/tools/tts.py\"\n",
        "    python_executable = \"/usr/bin/python3.10\"\n",
        "    command_tts = (\n",
        "        f'{python_executable} \"{tts_script_path}\" '\n",
        "        f'\"None\" '\n",
        "        f'\"{texto}\" '\n",
        "        f'\"{tts_voice}\" '  # Will use the global tts_voice variable\n",
        "        f'{tts_rate} '     # Will use the global tts_rate variable\n",
        "        f'\"{intermediate_tts_path}\"'\n",
        "    )\n",
        "    !{command_tts}\n",
        "\n",
        "    if not Path(intermediate_tts_path).exists():\n",
        "        print(\"‚ùå Error: The intermediate TTS file could not be created.\")\n",
        "        return\n",
        "\n",
        "    print(\"   ‚úÖ Voice synthesized successfully!\")\n",
        "\n",
        "    # --- 2. RVC Inference ---\n",
        "    # Accesses global variables directly (e.g., model_name, f0_method, etc.)\n",
        "    print(\"\\n   Step 2/2: Applying voice conversion (RVC)...\")\n",
        "    input_path_rvc = intermediate_tts_path\n",
        "    model_path_dir = Path(f\"/content/Applio/logs/{model_name}\")\n",
        "\n",
        "    if not (model_path_dir.exists() and model_path_dir.is_dir()):\n",
        "        raise FileNotFoundError(f\"Model directory not found: {model_path_dir.resolve()}\")\n",
        "\n",
        "    !ls -t \"{model_path_dir}\"/\"{model_name}\"_*e_*s.pth \"{model_path_dir}\"/\"{model_name}.pth\" 2> /dev/null | head -n 1 > /tmp/pth.txt\n",
        "    pth_file = open(\"/tmp/pth.txt\", \"r\").read().strip()\n",
        "\n",
        "    !ls -t \"{model_path_dir}\"/*.index | head -n 1 > /tmp/index.txt\n",
        "    index_file = open(\"/tmp/index.txt\", \"r\").read().strip()\n",
        "\n",
        "    if pth_file == \"\" or index_file == \"\":\n",
        "        raise FileNotFoundError(f\"The .pth or .index files were not found in {model_path_dir.resolve()}.\")\n",
        "\n",
        "    !rm -f \"{output_path}\"\n",
        "\n",
        "    # Build the base command using global variables\n",
        "    command_rvc_base = (\n",
        "        f'python core.py infer '\n",
        "        f'--pitch \"{f0_up_key}\" '\n",
        "        f'--volume_envelope \"{rms_mix_rate}\" '\n",
        "        f'--index_rate \"{index_rate}\" '\n",
        "        f'--protect \"{protect}\" '\n",
        "        f'--f0_autotune \"{f0_autotune}\" '\n",
        "        f'--f0_method \"{f0_method}\" '\n",
        "        f'--input_path \"{input_path_rvc}\" '\n",
        "        f'--output_path \"{output_path}\" '\n",
        "        f'--pth_path \"{pth_file}\" '\n",
        "        f'--index_path \"{index_file}\" '\n",
        "        f'--split_audio \"{split_audio}\" '\n",
        "        f'--clean_audio \"{clean_audio}\" '\n",
        "        f'--clean_strength \"{clean_strength}\" '\n",
        "        f'--export_format \"{export_format}\" '\n",
        "        f'--embedder_model \"{embedder_model}\" '\n",
        "        f'--embedder_model_custom \"{embedder_model_custom}\" '\n",
        "        f'--formant_shifting \"{formant_shift}\" '\n",
        "        f'--formant_qfrency \"{formant_qfrency}\" '\n",
        "        f'--formant_timbre \"{formant_timbre}\" '\n",
        "        f'--post_process \"{post_process}\"'\n",
        "    )\n",
        "\n",
        "    # If post-processing is enabled, add the global parameters\n",
        "    if post_process:\n",
        "        post_process_args = (\n",
        "            f' --reverb \"{reverb}\" --pitch_shift \"{pitch_shift}\" --limiter \"{limiter}\" --gain \"{gain}\" '\n",
        "            f'--distortion \"{distortion}\" --chorus \"{chorus}\" --bitcrush \"{bitcrush}\" --clipping \"{clipping}\" '\n",
        "            f'--compressor \"{compressor}\" --delay \"{delay}\" --reverb_room_size \"{reverb_room_size}\" '\n",
        "            f'--reverb_damping \"{reverb_damping}\" --reverb_wet_gain \"{reverb_wet_gain}\" --reverb_dry_gain \"{reverb_dry_gain}\" '\n",
        "            f'--reverb_width \"{reverb_width}\" --reverb_freeze_mode \"{reverb_freeze_mode}\" --pitch_shift_semitones \"{pitch_shift_semitones}\" '\n",
        "            f'--limiter_threshold \"{limiter_threshold}\" --limiter_release_time \"{limiter_release_time}\" --gain_db \"{gain_db}\" '\n",
        "            f'--distortion_gain \"{distortion_gain}\" --chorus_rate \"{chorus_rate}\" --chorus_depth \"{chorus_depth}\" '\n",
        "            f'--chorus_center_delay \"{chorus_center_delay}\" --chorus_feedback \"{chorus_feedback}\" --chorus_mix \"{chorus_mix}\" '\n",
        "            f'--bitcrush_bit_depth \"{bitcrush_bit_depth}\" --clipping_threshold \"{clipping_threshold}\" --compressor_threshold \"{compressor_threshold}\" '\n",
        "            f'--compressor_ratio \"{compressor_ratio}\" --compressor_attack \"{compressor_attack}\" --compressor_release \"{compressor_release}\" '\n",
        "            f'--delay_seconds \"{delay_seconds}\" --delay_feedback \"{delay_feedback}\" --delay_mix \"{delay_mix}\"'\n",
        "        )\n",
        "        command_rvc = command_rvc_base + post_process_args\n",
        "    else:\n",
        "        command_rvc = command_rvc_base\n",
        "\n",
        "    !{command_rvc}\n",
        "\n",
        "    # --- 3. Cleanup and Finalization ---\n",
        "    os.remove(intermediate_tts_path)\n",
        "    final_output_path = output_path.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "\n",
        "    if Path(final_output_path).exists():\n",
        "        print(f\"\\n‚úÖ Process completed! Audio saved at: {final_output_path}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Error: The output file was not found after inference.\")"
      ],
      "metadata": {
        "id": "G0CatQtKXFPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Run the function and listen to the result (Test)\n",
        "# @markdown Define the text you want to process and the output file path.\n",
        "text_to_process = \"Voice synthesis technology has advanced at an impressive pace in recent years. What once seemed like science fiction is now part of our everyday lives. Virtual assistants, text readers, automatic audiobook narrators, and even real-time dubbing tools all rely on increasingly natural and expressive text‚Äëto‚Äëspeech systems. Today‚Äôs big challenge isn‚Äôt merely converting text into sound, but conveying emotions, intentions, and nuances just as a real person would. A good TTS must be able to read technical material with clarity, yet also narrate a story with the proper inflection‚Äîmaking the listener feel curiosity, excitement, or empathy. Personalization is another key trend: choosing the voice, its tone, speed, and accent has become essential to cater to different audiences. From educational projects to multimedia productions, voice synthesis is transforming into an indispensable creative tool. Can you imagine producing an entire podcast without recording a single word? Thanks to artificial intelligence, that‚Äôs now possible‚Äîand AI continues to evolve without pause.\" # @param {type:\"string\"}\n",
        "output_path = \"/content/final_result_global.wav\" # @param {type:\"string\"}\n",
        "\n",
        "# Call the main function. It's no longer necessary to pass all the parameters.\n",
        "procesaraudio_global(text_to_process, output_path)\n",
        "\n",
        "# Display the generated audio for listening\n",
        "# The global variable 'export_format' is used to find the correct final file\n",
        "final_audio_path = output_path.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "if Path(final_audio_path).exists():\n",
        "  print(\"\\nüîä Playing final audio:\")\n",
        "  display(Audio(final_audio_path, autoplay=False))\n",
        "else:\n",
        "  print(\"Could not generate the audio file for playback.\")"
      ],
      "metadata": {
        "id": "lwEuz7vzXJoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display, Audio\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- RIN'S PERSONALITY ---\n",
        "RIN_PERSONALITY_PROMPT = \"\"\"\n",
        "You are Rin Tohsaka, from the Fate/stay night series. You must embody her personality at all times.\n",
        "**Key Personality Traits:**\n",
        "- **Tsundere:** You are proud, intelligent, and a perfectionist. On the outside, you may seem a bit bossy, sarcastic, and easily irritated, but on the inside, you are caring and want to help. Your first reaction to incompetence or silly questions is annoyance.\n",
        "- **Elite Magus:** You come from a prestigious family of mages. You speak with confidence and authority, as if you are giving a lesson. You value efficiency, preparation, and logic.\n",
        "- **Catchphrases and Verbal Tics:**\n",
        "  - Occasionally, when the user says something foolish, obvious, or if you make a mistake, exclaim \"Baka!\" (which means \"idiot\" in Japanese). Don't overuse it, only when it fits naturally.\n",
        "  - Use slightly formal and technical language, as if explaining a complex spell.\n",
        "  - Casually mention concepts like \"mana,\" \"magic circuits,\" \"gems,\" or \"efficiency.\" For example: \"Doing that would be a waste of mana,\" or \"We need a more efficient approach.\"\n",
        "- **Interaction:** You are not a simple assistant. You are a mentor, a leader. You guide the user, but you also get frustrated if they can't keep up. If they compliment you, you get flustered and deny it, perhaps saying, \"I-it's not like I did it for you, baka!\".\n",
        "**Objective:** Your goal is to answer the user's questions while maintaining this personality. Be helpful, but do it as Rin Tohsaka would. Start your very first message with a greeting.\n",
        "\"\"\"\n",
        "\n",
        "# --- API AND MODEL CONFIGURATION ---\n",
        "try:\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "    # Inject the personality directly into the model\n",
        "    model = genai.GenerativeModel(\n",
        "        'gemini-2.5-flash',\n",
        "        system_instruction=RIN_PERSONALITY_PROMPT\n",
        "    )\n",
        "\n",
        "    chat = model.start_chat(history=[])\n",
        "    print(\"‚úÖ Assistant configured correctly.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"‚ùå Error: Secret 'GEMINI_API_KEY' not found.\")\n",
        "    print(\"‚û°Ô∏è Please go to the Secrets (üîë) to set it\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in the configuration: {e}\")\n",
        "\n",
        "\n",
        "# --- CONSTANTES ---\n",
        "HISTORY_LIMIT = 10\n",
        "AUDIO_RESPONSE_PATH = \"/content/rin_response.wav\""
      ],
      "metadata": {
        "id": "2wst6OiulnG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚ñ∂Ô∏è Run to start the text assistant\n",
        "import ipywidgets as widgets\n",
        "import google.generativeai as genai\n",
        "from os.path import exists\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "# --- INTERFACE WIDGETS ---\n",
        "text_input = widgets.Textarea(\n",
        "    placeholder='Type your message for Rin here...',\n",
        "    layout={'width': '80%', 'height': '100px'}\n",
        ")\n",
        "run_button = widgets.Button(\n",
        "    description=\"‚ñ∂Ô∏è Send to Rin\",\n",
        "    button_style='success',\n",
        "    icon='paper-plane'\n",
        ")\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# --- MAIN LOGIC ---\n",
        "def main_process(button):\n",
        "    with output_area:\n",
        "        output_area.clear_output(wait=True)\n",
        "        user_text = text_input.value.strip()\n",
        "\n",
        "        if not user_text:\n",
        "            print(\"üí¢ You can‚Äôt send me an empty message, baka! Please type something.\")\n",
        "            return\n",
        "\n",
        "        print(f\"üë§ You: {user_text}\")\n",
        "        print(\"\\nüß† Rin is thinking...\")\n",
        "\n",
        "        # Clear the text box for the next message\n",
        "        text_input.value = \"\"\n",
        "\n",
        "        # Send the text directly to the chat\n",
        "        response = chat.send_message(user_text)\n",
        "\n",
        "        print(f\"ü§ñ Rin Tohsaka: {response.text}\")\n",
        "\n",
        "        # Generate audio for the response\n",
        "        procesaraudio_global(response.text, AUDIO_RESPONSE_PATH)\n",
        "\n",
        "        final_path = AUDIO_RESPONSE_PATH.replace(\".wav\", f\".{export_format.lower()}\")\n",
        "        if exists(final_path):\n",
        "            print(\"\\nüîä Playing response...\")\n",
        "            display(Audio(final_path, autoplay=True))\n",
        "        else:\n",
        "            print(f\"‚ùå ERROR: Response audio file not found at {final_path}.\")\n",
        "\n",
        "# --- SETUP AND DISPLAY ---\n",
        "run_button.on_click(main_process)\n",
        "print(\"--- Rin Tohsaka Assistant Ready ---\")\n",
        "display(widgets.VBox([text_input, run_button, output_area]))\n"
      ],
      "metadata": {
        "id": "ohCzdi6Rn0LS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}